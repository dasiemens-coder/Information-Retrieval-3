{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoincRanker Notebook Overview\n",
    "\n",
    "This notebook implements a learning-to-rank pipeline for LOINC data using an AdaRank model.  It covers:\n",
    "- Data merging from multi-sheet Excel files,\n",
    "- Diverse feature extraction techniques (lexical, semantic, and numerical),\n",
    "- Construction of a combined sparse feature matrix,\n",
    "- Query-aware model training and evaluation,\n",
    "- And finally, analysis of model results.\n",
    "\n",
    "The underlying AdaRank model is implemented using the repository git@github.com:rueycheng/AdaRank.git. The AdaRank algorithm was modified to include extra regularization mechnisms. \n",
    "\n",
    "Example predictions can be found in the output of the cells and in the folder Results/.\n",
    "\n",
    "---\n",
    "\n",
    "## Content of notebook \n",
    "\n",
    "1. Data Loading \n",
    "2. Feature Extraction, Data Merging\n",
    "3. Indexing\n",
    "4. AdaRank Training\n",
    "5. AdaRank Evaluation \n",
    "6. Example Retrivals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from adarankv2 import AdaRankv2\n",
    "from metrics import NDCGScorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import save_npz, load_npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" val_file_path = 'Input-test/loinc_query_terms_testing.xlsx'\n",
    "train_file_path = 'Input-test/loinc_query_terms_training.xlsx' \"\"\"\n",
    "val_file_path = 'Input/loinc_query_terms_testing.xlsx'\n",
    "train_file_path = 'Input/loinc_query_terms_training.xlsx'\n",
    "\n",
    "# Load pretrained model vector embedding model\n",
    "# We are using a medical specific model from huggingface. https://huggingface.co/ls-da3m0ns/bge_large_medical 'ls-da3m0ns/bge_large_medical'\n",
    "# To improve performance, model can be switched to a more general & lightwight model (replace with \"all-MiniLM-L6-v2\")\n",
    "\n",
    "embedder = SentenceTransformer('ls-da3m0ns/bge_large_medical')\n",
    "#embedder = SentenceTransformer('all-MiniLM-L6-v2') # lightweight model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define function for semantic similarity computation\n",
    "We use a pretrained embedding model for semantic similarity features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely calculate similarity between a query and a field\n",
    "def calculate_embedding_similarity(query, field):\n",
    "    if pd.isna(query) or pd.isna(field):\n",
    "        return 0\n",
    "    query_embedding = embedder.encode([str(query)])[0]\n",
    "    field_embedding = embedder.encode([str(field)])[0]\n",
    "    return cosine_similarity([query_embedding], [field_embedding])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Data Loading, Feature Extraction, Data Merging\n",
    "In this cell we select and create the weak rankers for AdaRank\n",
    "\n",
    "- **Data Processing**\n",
    "  - Reads multiple sheets from an Excel file, adding each sheet name as a 'query' column\n",
    "  - Merges all sheet data into a single DataFrame for unified processing\n",
    "\n",
    "- **Feature Engineering: Lexical Similarity**\n",
    "  - Jaccard Similarity: Calculates word-level overlap between query and medical attributes property, system, component\n",
    "  - TF-IDF Cosine Similarity: Computes similarity between query and long_common_name using TF-IDF vectorization\n",
    "\n",
    "- **Feature Engineering: Semantic Similarity**\n",
    "  - Calculates embedding-based similarity between query and string fields. \n",
    "\n",
    "- **Numerical Feature Processing**\n",
    "  - Normalizes the 'rank' feature using StandardScaler\n",
    "  - Replaces zero values with values higher than maximum rank\n",
    "\n",
    "- **Feature Combination**\n",
    "  - Horizontally stacks all sparse feature matrices\n",
    "  - Prepares target variable from 'relevant' column\n",
    "  - Converts query strings to numeric IDs for compatibility with ranking algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file and merge all sheets into one dataframe\n",
    "xls = pd.ExcelFile(train_file_path)\n",
    "dataframes = []\n",
    "for sheet_name in xls.sheet_names:\n",
    "    temp_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "    temp_df['query'] = sheet_name  # Each sheet name is used as the query text\n",
    "    dataframes.append(temp_df)\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Compute jaccard similarity for property, system, and component\n",
    "\n",
    "# Define a function to compute the Jaccard similarity between two strings.\n",
    "def jaccard_similarity(str1, str2):\n",
    "    # If either string is not a valid string, return 0.\n",
    "    if not isinstance(str1, str) or not isinstance(str2, str):\n",
    "        return 0.0\n",
    "    set1 = set(str1.lower().split())\n",
    "    set2 = set(str2.lower().split())\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return float(len(set1.intersection(set2))) / len(union)\n",
    "\n",
    "# Compute Jaccard similarity for each of the  columns, comparing the 'query' to each.\n",
    "merged_df['system_jaccard'] = merged_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['system']) if pd.notnull(row['system']) else 0.0, axis=1)\n",
    "merged_df['component_jaccard'] = merged_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['component']) if pd.notnull(row['component']) else 0.0, axis=1)\n",
    "merged_df['property_jaccard'] = merged_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['property']) if pd.notnull(row['property']) else 0.0, axis=1)\n",
    "\n",
    "# inspect the computed similarities (not needed for training)\n",
    "\"\"\" print(\"System Jaccard similarity stats:\")\n",
    "print(merged_df['system_jaccard'].describe())\n",
    "print(\"Component Jaccard similarity stats:\")\n",
    "print(merged_df['component_jaccard'].describe())\n",
    "print(\"Property Jaccard similarity stats:\")\n",
    "print(merged_df['property_jaccard'].describe()) \"\"\"\n",
    "\n",
    "# Convert the columns to sparse matrices so they can be used as features.\n",
    "from scipy.sparse import csr_matrix\n",
    "X_system_jaccard = csr_matrix(merged_df[['system_jaccard']].values)\n",
    "X_component_jaccard = csr_matrix(merged_df[['component_jaccard']].values)\n",
    "X_property_jaccard = csr_matrix(merged_df[['property_jaccard']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute lexical similarity for long_common_name\n",
    "\n",
    "# We want to compute, for each row, the similarity between the query and the long_common_name.\n",
    "# First, build a TF-IDF vectorizer fitted on the union of all queries and names.\n",
    "corpus = pd.concat([merged_df['query'], merged_df['long_common_name']])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the query and long_common_name columns\n",
    "X_query = vectorizer.transform(merged_df['query'])\n",
    "X_name_tfidf = vectorizer.transform(merged_df['long_common_name'])\n",
    "\n",
    "# Compute cosine similarity for each row\n",
    "cosine_sim = np.array([cosine_similarity(X_query[i], X_name_tfidf[i])[0, 0] \n",
    "                         for i in range(X_query.shape[0])])\n",
    "merged_df['name_cosine_sim'] = cosine_sim\n",
    "\n",
    "# Now use the computed cosine similarity as the feature for the name field.\n",
    "X_name_lexical_similarity = csr_matrix(merged_df[['name_cosine_sim']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute semantic similarity for features property, system, component, long_common_name\n",
    "\n",
    "# Create similarity scores for features, loinc name measurement, system, and component\n",
    "# Apply a similarity function to each row, based on vector embeddings \n",
    "merged_df['name_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['long_common_name']), axis=1)\n",
    "merged_df['property_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['property']), axis=1)\n",
    "merged_df['system_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['system']), axis=1)\n",
    "merged_df['component_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['component']), axis=1)\n",
    "\n",
    "# Convert to sparse matrices\n",
    "X_name_semantic_similarity = csr_matrix(merged_df[['name_similarity']].values)\n",
    "X_property_similarity = csr_matrix(merged_df[['property_similarity']].values)\n",
    "X_system_similarity = csr_matrix(merged_df[['system_similarity']].values)\n",
    "X_component_similarity = csr_matrix(merged_df[['component_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Process numerical feature: 'rank' using StandardScaler\n",
    "# Treat 0 (NaN) values as very high ranks by replacing them with the largest number \n",
    "max_rank = merged_df['rank'].max()\n",
    "merged_df['rank'] = merged_df['rank'].replace(0, max_rank + 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_rank = scaler.fit_transform(merged_df[['rank']])\n",
    "X_rank_sparse = csr_matrix(X_rank)\n",
    "\n",
    "# ------------------------------\n",
    "# Combine all features into one sparse matrix.\n",
    "X = hstack([\n",
    "    X_rank_sparse,\n",
    "    X_name_semantic_similarity, \n",
    "    X_name_lexical_similarity,\n",
    "    X_system_similarity, \n",
    "    X_component_similarity,\n",
    "    X_system_jaccard,\n",
    "    X_component_jaccard,\n",
    "    X_property_jaccard\n",
    "])\n",
    "\n",
    "# Labels and query identifiers\n",
    "y = merged_df['relevant'].values\n",
    "# convert query strings to integers -> required for AdaRank\n",
    "merged_df['qid_numeric'] = pd.factorize(merged_df['query'])[0]\n",
    "qid = merged_df['qid_numeric'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Indexing\n",
    "Store processed features in /idxdata folder for faster retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('idxdata/indexed_dataset.csv', index=False)\n",
    "# Save the feature matrix X to a file for later retrieval\n",
    "save_npz('idxdata/X_sparse.npz', X)\n",
    "# Save the labels y to a file for later retrieval\n",
    "np.save('idxdata/y.npy', y)\n",
    "# Save the query identifiers qid to a file for later retrieval\n",
    "np.save('idxdata/qid.npy', qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = load_npz('idxdata/X_sparse.npz')\n",
    "merged_df = pd.read_csv('idxdata/indexed_dataset.csv')\n",
    "y = np.load('idxdata/y.npy')\n",
    "qid = np.load('idxdata/qid.npy') \n",
    "# Create a string array capturing the name of the features -> used later during evaluation\n",
    "feature_names = [\n",
    "    \"X_rank_sparse\",\n",
    "    \"X_name_semantic_similarity\", \n",
    "    \"X_name_lexical_similarity\",\n",
    "    \"X_system_similarity\", \n",
    "    \"X_component_similarity\",\n",
    "    \"X_system_jaccard\",\n",
    "    \"X_component_jaccard\",\n",
    "    \"X_property_jaccard\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. AdaRank Training\n",
    "- Train AdaRank on the previouosly generated data.\n",
    "\n",
    "\n",
    "\n",
    "- **Dataset Splitting**\n",
    "  - Uses GroupShuffleSplit to create train/test sets (80/20 split)\n",
    "  - Maintains query integrity by keeping all documents for a query together\n",
    "  - We do not use k-fold as during AdaRank this would lead to new model initialization for every fold -> no transferability.\n",
    "\n",
    "- **Split Verification**\n",
    "  - Creates separate DataFrames for train and test data\n",
    "\n",
    "- **Feature Validation**\n",
    "  - Calculates basic statistics (mean, standard deviation) for features\n",
    "  - Confirms meaningful variance exists across all features\n",
    "\n",
    "- **Model Training**\n",
    "  - Implements AdaRank with 500 maximum iterations, and 50 iterations early stoppage \n",
    "\n",
    "- **Preliminiary Model Evaluation**\n",
    "  - Tests model performance across multiple ranking depths (k=1,2,3,4,5,10,20)\n",
    "  - Calculates and reports NDCG score at each depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique queries in train: [0 1 3 5 6 7]\n",
      "Unique queries in test: [2 4]\n",
      "Unique queries and their frequencies: (array([0, 1, 2, 3, 4, 5, 6, 7]), array([517, 491, 517, 514, 550, 521, 550, 550]))\n",
      "X_train summary stats (mean, std): [0.0056889  0.39012529 0.0403473  0.36812505 0.38595793 0.00403012\n",
      " 0.01871622 0.05472479] [0.95613794 0.11574428 0.08996267 0.08048696 0.12437721 0.03642981\n",
      " 0.1132546  0.22744226]\n",
      "y_train summary stats (mean, std): 0.10976773783009863 0.31260003448778023\n",
      "NDCG Score 1.0, K 1\n",
      "NDCG Score 1.0, K 2\n",
      "NDCG Score 1.0, K 3\n",
      "NDCG Score 1.0, K 4\n",
      "NDCG Score 1.0, K 5\n",
      "NDCG Score 1.0, K 10\n",
      "NDCG Score 1.0, K 20\n"
     ]
    }
   ],
   "source": [
    "# Query-aware train/test splitting\n",
    "# we disbale te random state due to the small dataset\n",
    "splitter = GroupShuffleSplit(test_size=0.2, random_state=None)\n",
    "train_idx, test_idx = next(splitter.split(X, y, groups=qid))\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "qid_train, qid_test = qid[train_idx], qid[test_idx]\n",
    "\n",
    "# Create DataFrames for the train and test sets using the respective indices\n",
    "train_df = merged_df.iloc[train_idx].copy()\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "\n",
    "#Debugging prints\n",
    "print(\"Unique queries in train:\", np.unique(qid[train_idx]))\n",
    "print(\"Unique queries in test:\", np.unique(qid[test_idx]))\n",
    "print(\"Unique queries and their frequencies:\", np.unique(qid, return_counts=True))\n",
    "\n",
    "#print(\"X_train:\\n\", X_train.toarray())\n",
    "#print(\"y_train:\", y_train)\n",
    "\n",
    "#Check basic statistics \n",
    "#We can confirm that all selected features plus label have a meaningful variance/std dev so we dont need to apply a feature selection before \n",
    "print(\"X_train summary stats (mean, std):\", np.mean(X_train.toarray(), axis=0), np.std(X_train.toarray(), axis=0))\n",
    "#print(\"y_train distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"y_train summary stats (mean, std):\", np.mean(y_train, axis=0), np.std(y_train, axis=0))\n",
    "\n",
    "# ------------------------------\n",
    "# Train and evaluate AdaRank\n",
    "model = AdaRankv2(max_iter=500, estop=50, scorer=NDCGScorer(k=5))\n",
    "#model = AdaRank(max_iter=100, estop=100, scorer=NDCGScorer(k=5))\n",
    "model.fit(X_train, y_train, qid_train)\n",
    "\n",
    "# test NNDCG for different values of k\n",
    "for k in (1, 2, 3, 4, 5, 10, 20):\n",
    "    y_pred = model.predict(X_test, qid_test) \n",
    "    score = NDCGScorer(k=k)(y_test, y_pred, qid_test).mean()   \n",
    "    print(f\"NDCG Score {score}, K {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. AdaRank Evaluation \n",
    "- **Test Results Setup**: Creates test DataFrame with predictions and true labels; counts unique queries in test set and full dataset to verify coverage\n",
    "\n",
    "- **Prediction Analysis**: Groups and sorts results by prediction score; displays top 10 predictions per query with item names, true relevance, and predicted scores\n",
    "\n",
    "- **Display Coefficient**: We diplay the coefficient for AdaRank. AdaRank does only select two featrues for generating the ranking. This behavious was consistent even after impelemnting additional regularization in AdaRank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique queries in test: 2\n",
      "[2 4]\n",
      "Number of distinct queries overall: 8\n",
      "['glucose in blood' 'bilirubin in plasma' 'White blood cells count'\n",
      " 'cholesterol in Bld' 'fever virus' 'calcium oxalate crystals' 'iron'\n",
      " 'PrThr']\n",
      "Top Predictions per Query:\n",
      "Query: White blood cells count\n",
      "                                       long_common_name  y_true    y_pred\n",
      "1098  Blasts/Leukocytes [Pure number fraction] in Bo...       1  4.701988\n",
      "1088  Deprecated Myeloblasts/100 leukocytes in Blood...       1  4.667403\n",
      "1122  Myelocytes/Leukocytes in Stem cell product by ...       1  4.658710\n",
      "1112  Deprecated Monocytes+Macrophages/100 leukocyte...       1  4.655984\n",
      "1097          Large unstained cells/Leukocytes in Blood       1  4.650530\n",
      "1079          Nonhematic cells/Leukocytes in Body fluid       1  4.629085\n",
      "1121  Eosinophils/Leukocytes in Stem cell product by...       1  4.628964\n",
      "1105                    Heterophils/Leukocytes in Blood       1  4.614963\n",
      "1095  Other cells/Leukocytes in Synovial fluid by Ma...       1  4.610029\n",
      "1081  Neutrophils/Leukocytes [Pure number fraction] ...       1  4.604194\n",
      "----------------------------------------\n",
      "Query: fever virus\n",
      "                                       long_common_name  y_true    y_pred\n",
      "2083  Colorado tick fever virus Ab [Presence] in Spe...       1  4.585992\n",
      "2046  Rift valley fever virus Ag [Presence] in Specimen       1  4.575512\n",
      "2076     Rift valley fever virus Ab [Presence] in Serum       1  4.572767\n",
      "2075  Colorado tick fever virus RNA [Presence] in Sp...       1  4.569263\n",
      "2052  African swine fever virus Ab [Presence] in Bod...       1  4.553313\n",
      "2082  African swine fever virus Ag [Presence] in Tis...       1  4.550629\n",
      "2042  Colorado tick fever virus IgG Ab [Presence] in...       1  4.549562\n",
      "2085          Yellow fever virus Ab [Presence] in Serum       1  4.547721\n",
      "2078  Crimean-Congo hemorrhagic fever virus RNA [Pre...       1  4.547554\n",
      "2059  Classical swine fever virus neutralizing antib...       1  4.542923\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Showcasing some predictions from the test set\n",
    "\n",
    "# Create a DataFrame for the test set using the test indices\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "test_df['y_true'] = y_test\n",
    "test_df['y_pred'] = y_pred\n",
    "\n",
    "#Check how many distinct queries are in the test set\n",
    "unique_test_queries = np.unique(qid_test)\n",
    "print(\"Number of unique queries in test:\", len(unique_test_queries))\n",
    "print(unique_test_queries)\n",
    "\n",
    "all_queries = merged_df['query'].unique()\n",
    "print(\"Number of distinct queries overall:\", len(all_queries))\n",
    "print(all_queries)\n",
    "\n",
    "# for each query in the test set, print the top 10 predictions (sorted by predicted score)\n",
    "# Model seems to predcit well, however could be due to the small dataset and overfitting \n",
    "print(\"Top Predictions per Query:\")\n",
    "for query, group in test_df.groupby('query'):\n",
    "    sorted_group = group.sort_values(by='y_pred', ascending=False)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(sorted_group[['long_common_name', 'y_true', 'y_pred']].head(10))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.20870325 1.23835749 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Feature: X_rank_sparse, Importance (coef): 1.2087\n",
      "Feature: X_name_semantic_similarity, Importance (coef): 1.2384\n",
      "Feature: X_name_lexical_similarity, Importance (coef): 0.0000\n",
      "Feature: X_system_similarity, Importance (coef): 0.0000\n",
      "Feature: X_component_similarity, Importance (coef): 0.0000\n",
      "Feature: X_system_jaccard, Importance (coef): 0.0000\n",
      "Feature: X_component_jaccard, Importance (coef): 0.0000\n",
      "Feature: X_property_jaccard, Importance (coef): 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Check how the model weights the features \n",
    "coef_zip = model.coef_\n",
    "# Print feature coefficients explicitly\n",
    "print(coef_zip)\n",
    "for name, coef in zip(feature_names, coef_zip):\n",
    "    print(f\"Feature: {name}, Importance (coef): {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Example Predictions\n",
    "- **Validation Data Processing**: Loads Excel validation data with sheet names as queries; computes the same feature set used in training (lexical similarities, semantic similarities, and Jaccard scores for various fields)\n",
    "\n",
    "- **Prediction & Output**: Do example predictions on three queries on a different example dataset and store it under \"Results/\" folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refactor code to prepare data \n",
    "xls_val = pd.ExcelFile(val_file_path)\n",
    "dataframes_val = []\n",
    "for sheet_name in xls_val.sheet_names:\n",
    "    temp_df = pd.read_excel(xls_val, sheet_name=sheet_name)\n",
    "    temp_df['query'] = sheet_name  # Each sheet name is the query text\n",
    "    dataframes_val.append(temp_df)\n",
    "val_df = pd.concat(dataframes_val, ignore_index=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute Lexical Similarity for 'long_common_name'\n",
    "\n",
    "corpus = pd.concat([val_df['query'], val_df['long_common_name']])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "X_query_val = vectorizer.transform(val_df['query'])\n",
    "X_name_tfidf_val = vectorizer.transform(val_df['long_common_name'])\n",
    "cosine_sim_val = np.array([\n",
    "    cosine_similarity(X_query_val[i], X_name_tfidf_val[i])[0, 0] \n",
    "    for i in range(X_query_val.shape[0])\n",
    "])\n",
    "val_df['name_lexical_similarity'] = cosine_sim_val\n",
    "X_name_lexical_similarity_val = csr_matrix(val_df[['name_lexical_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute Lexical Similarity \n",
    "\n",
    "\n",
    "# Compute Jaccard similarity for each of the  columns, comparing the 'query' to each.\n",
    "val_df['component_jaccard'] = val_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['component']) if pd.notnull(row['component']) else 0.0, axis=1)\n",
    "val_df['system_jaccard'] = val_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['system']) if pd.notnull(row['system']) else 0.0, axis=1)\n",
    "val_df['property_jaccard'] = val_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['property']) if pd.notnull(row['property']) else 0.0, axis=1)\n",
    "\n",
    "# Convert the columns to sparse matrices so they can be used as features.\n",
    "from scipy.sparse import csr_matrix\n",
    "X_component_jaccard_val = csr_matrix(val_df[['component_jaccard']].values)\n",
    "X_system_jaccard_val = csr_matrix(val_df[['system_jaccard']].values)\n",
    "X_property_jaccard_val = csr_matrix(val_df[['property_jaccard']].values)\n",
    "# ------------------------------\n",
    "# Compute Semantic Similarities\n",
    "\n",
    "val_df['name_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['long_common_name']), axis=1)\n",
    "val_df['property_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['property']), axis=1)\n",
    "val_df['system_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['system']), axis=1)\n",
    "val_df['component_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['component']), axis=1)\n",
    "\n",
    "X_name_semantic_similarity_val = csr_matrix(val_df[['name_semantic_similarity']].values)\n",
    "X_property_similarity_val = csr_matrix(val_df[['property_semantic_similarity']].values)\n",
    "X_system_similarity_val = csr_matrix(val_df[['system_semantic_similarity']].values)\n",
    "X_component_similarity_val = csr_matrix(val_df[['component_semantic_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Process Numerical Feature: 'rank'\n",
    "# Replace zeros (or NaN) with a high rank value as before\n",
    "max_rank_val = val_df['rank'].max()\n",
    "val_df['rank'] = val_df['rank'].replace(0, max_rank_val + 1)\n",
    "# Use the same scaler from training.\n",
    "scaler_val = StandardScaler().fit(merged_df[['rank']])\n",
    "X_rank_val = scaler_val.transform(val_df[['rank']])\n",
    "X_rank_sparse_val = csr_matrix(X_rank_val)\n",
    "\n",
    "# ------------------------------\n",
    "# Combine All Features into One Matrix\n",
    "# Use the same ordering as during training.\n",
    "X_val = hstack([\n",
    "    X_rank_sparse_val,\n",
    "    X_name_semantic_similarity_val, \n",
    "    X_name_lexical_similarity_val,\n",
    "    X_system_similarity_val, \n",
    "    X_component_similarity_val,\n",
    "    X_system_jaccard_val,\n",
    "    X_component_jaccard_val,\n",
    "    X_property_jaccard_val\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' print(\"Validation Predictions:\")\\nfor query, group in val_df.groupby(\\'query\\'):\\n    print(f\"\\nQuery: {query}\")\\n    # Adjust column names as needed; here we print the document \\'long_common_name\\' and its prediction.\\n    print(group[[\\'long_common_name\\', \\'y_pred\\']])  '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['qid_numeric'] = pd.factorize(val_df['query'])[0]\n",
    "qid_val = val_df['qid_numeric'].values\n",
    "\n",
    "# Predict Using the Fitted AdaRank Model\n",
    "# Assume your AdaRank model has already been trained and is available as 'model'\n",
    "y_val_pred = model.predict(X_val, qid_val)\n",
    "\n",
    "# Add the predictions to the DataFrame for inspection.\n",
    "val_df['y_pred'] = y_val_pred\n",
    "# Print Example Predictions Grouped by Query\n",
    "\"\"\" print(\"Validation Predictions:\")\n",
    "for query, group in val_df.groupby('query'):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    # Adjust column names as needed; here we print the document 'long_common_name' and its prediction.\n",
    "    print(group[['long_common_name', 'y_pred']])  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions:\n",
      "\n",
      "Query: Bacteria Flue\n",
      "                                       long_common_name    y_pred\n",
      "1000  11-Deoxycortisol [Mass/volume] in Serum or Plasma  0.378464\n",
      "1001  Hemoglobin.gastrointestinal [Presence] in Vomitus  0.679689\n",
      "1002          Busulfan [Mass/volume] in Serum or Plasma  0.805167\n",
      "1003  Fetal Trisomy 13 risk [Likelihood] based on Pl...  1.599803\n",
      "1004  Cholesterol esters/Cholesterol.total in Serum ... -0.776623\n",
      "...                                                 ...       ...\n",
      "1495  Helicobacter pylori Ab [Presence] in Serum by ...  1.841552\n",
      "1496  Decanoylcarnitine (C10) [Moles/volume] in Seru... -0.253396\n",
      "1497  Prothrombin time (PT) in Control Platelet poor...  0.434744\n",
      "1498  Microalbumin [Mass/time] in Urine collected fo...  0.841374\n",
      "1499    Bordetella pertussis IgG Ab [Presence] in Serum  0.700706\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "\n",
      "Query: Cholesterol in Blood\n",
      "                                      long_common_name    y_pred\n",
      "500  11-Deoxycortisol [Mass/volume] in Serum or Plasma  0.668452\n",
      "501  Hemoglobin.gastrointestinal [Presence] in Vomitus  0.823037\n",
      "502          Busulfan [Mass/volume] in Serum or Plasma  1.000234\n",
      "503  Fetal Trisomy 13 risk [Likelihood] based on Pl...  1.714835\n",
      "504  Cholesterol esters/Cholesterol.total in Serum ... -0.171250\n",
      "..                                                 ...       ...\n",
      "995  Helicobacter pylori Ab [Presence] in Serum by ...  1.886297\n",
      "996  Decanoylcarnitine (C10) [Moles/volume] in Seru...  0.048539\n",
      "997  Prothrombin time (PT) in Control Platelet poor...  0.669631\n",
      "998  Microalbumin [Mass/time] in Urine collected fo...  1.034135\n",
      "999    Bordetella pertussis IgG Ab [Presence] in Serum  0.782561\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "\n",
      "Query: Creatinine Urine or Blood\n",
      "                                      long_common_name    y_pred\n",
      "0    11-Deoxycortisol [Mass/volume] in Serum or Plasma  0.598253\n",
      "1    Hemoglobin.gastrointestinal [Presence] in Vomitus  0.832570\n",
      "2            Busulfan [Mass/volume] in Serum or Plasma  1.069418\n",
      "3    Fetal Trisomy 13 risk [Likelihood] based on Pl...  1.687261\n",
      "4    Cholesterol esters/Cholesterol.total in Serum ... -0.485273\n",
      "..                                                 ...       ...\n",
      "495  Helicobacter pylori Ab [Presence] in Serum by ...  1.855611\n",
      "496  Decanoylcarnitine (C10) [Moles/volume] in Seru...  0.064030\n",
      "497  Prothrombin time (PT) in Control Platelet poor...  0.680079\n",
      "498  Microalbumin [Mass/time] in Urine collected fo...  1.233286\n",
      "499    Bordetella pertussis IgG Ab [Presence] in Serum  0.813297\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "val_df['qid_numeric'] = pd.factorize(val_df['query'])[0]\n",
    "qid_val = val_df['qid_numeric'].values\n",
    "\n",
    "# Predict Using the Fitted AdaRank Model\n",
    "# Assume your AdaRank model has already been trained and is available as 'model'\n",
    "y_val_pred = model.predict(X_val, qid_val)\n",
    "\n",
    "# Add the predictions to the DataFrame for inspection.\n",
    "val_df['y_pred'] = y_val_pred\n",
    "# Print Example Predictions Grouped by Query\n",
    "print(\"Validation Predictions:\")\n",
    "for query, group in val_df.groupby('query'):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    # Adjust column names as needed; here we print the document 'long_common_name' and its prediction.\n",
    "    print(group[['long_common_name', 'y_pred']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results saved to Results/validation_results_by_query.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Function to assign ranking within each query group\n",
    "def assign_ranking(group):\n",
    "    group = group.sort_values(by='y_pred', ascending=False).copy()\n",
    "    group['AdaRank Ranking'] = range(1, len(group) + 1)\n",
    "    return group\n",
    "\n",
    "# Create a dictionary where each key is a query and value is the ranked DataFrame for that query\n",
    "query_groups = {query: assign_ranking(group) for query, group in val_df.groupby('query')}\n",
    "\n",
    "# Write each query's results to a separate sheet in an Excel file\n",
    "output_file = 'Results/validation_results_by_query.xlsx'\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for query, df_group in query_groups.items():\n",
    "        # Excel sheet names can have a maximum of 31 characters, so we truncate if necessary\n",
    "        sheet_name = query if len(query) <= 31 else query[:31]\n",
    "        df_group.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Validation results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
