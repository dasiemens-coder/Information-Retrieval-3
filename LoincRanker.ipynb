{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davis/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from adarank import AdaRank\n",
    "from adarankv2 import AdaRankv2\n",
    "from metrics import NDCGScorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import save_npz, load_npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#define file path\n",
    "file_path = 'loinc_ranks_query_moreterms_v2.xlsx'\n",
    "\n",
    "# Load pretrained model vector embedding model\n",
    "# We are using a medical specific model from huggingface. https://huggingface.co/ls-da3m0ns/bge_large_medical 'ls-da3m0ns/bge_large_medical'\n",
    "# To improve performance, model can be swithced to a more general & lightwight model (replace with \"all-MiniLM-L6-v2\")\n",
    "# embedder = SentenceTransformer('ls-da3m0ns/bge_large_medical')\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely calculate similarity between a query and a field\n",
    "def calculate_embedding_similarity(query, field):\n",
    "    if pd.isna(query) or pd.isna(field):\n",
    "        return 0\n",
    "    query_embedding = embedder.encode([str(query)])[0]\n",
    "    field_embedding = embedder.encode([str(field)])[0]\n",
    "    return cosine_similarity([query_embedding], [field_embedding])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file and merge all sheets into one dataframe\n",
    "xls = pd.ExcelFile(file_path)\n",
    "dataframes = []\n",
    "for sheet_name in xls.sheet_names:\n",
    "    temp_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "    temp_df['query'] = sheet_name  # Each sheet name is used as the query text\n",
    "    dataframes.append(temp_df)\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute lexical similarity for long_common_name\n",
    "\n",
    "# We want to compute, for each row, the similarity between the query and the long_common_name.\n",
    "# First, build a TF-IDF vectorizer fitted on the union of all queries and names.\n",
    "corpus = pd.concat([merged_df['query'], merged_df['long_common_name']])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the query and long_common_name columns\n",
    "X_query = vectorizer.transform(merged_df['query'])\n",
    "X_name_tfidf = vectorizer.transform(merged_df['long_common_name'])\n",
    "\n",
    "# Compute cosine similarity for each row\n",
    "cosine_sim = np.array([cosine_similarity(X_query[i], X_name_tfidf[i])[0, 0] \n",
    "                         for i in range(X_query.shape[0])])\n",
    "merged_df['name_cosine_sim'] = cosine_sim\n",
    "\n",
    "# Now use the computed cosine similarity as the feature for the name field.\n",
    "X_name_lexical_similarity = csr_matrix(merged_df[['name_cosine_sim']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute lexical similarity for features property, system, component, long_common_name\n",
    "\n",
    "# Create similarity scores for features, loinc name measurement, system, and component\n",
    "# Apply a similarity function to each row, based on vector embeddings \n",
    "merged_df['name_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['long_common_name']), axis=1)\n",
    "merged_df['property_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['property']), axis=1)\n",
    "merged_df['system_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['system']), axis=1)\n",
    "merged_df['component_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['component']), axis=1)\n",
    "\n",
    "# Convert to sparse matrices\n",
    "X_name_semantic_similarity = csr_matrix(merged_df[['name_similarity']].values)\n",
    "X_property_similarity = csr_matrix(merged_df[['property_similarity']].values)\n",
    "X_system_similarity = csr_matrix(merged_df[['system_similarity']].values)\n",
    "X_component_similarity = csr_matrix(merged_df[['component_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Process numerical feature: 'rank' using StandardScaler\n",
    "# Treat 0 (NaN) values as very high ranks by replacing them with the largest number \n",
    "max_rank = merged_df['rank'].max()\n",
    "merged_df['rank'] = merged_df['rank'].replace(0, max_rank + 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_rank = scaler.fit_transform(merged_df[['rank']])\n",
    "X_rank_sparse = csr_matrix(X_rank)\n",
    "\n",
    "# ------------------------------\n",
    "# Combine all features into one sparse matrix.\n",
    "X = hstack(\n",
    "    [\n",
    "        X_name_lexical_similarity, \n",
    "        X_name_semantic_similarity, \n",
    "        X_system_similarity, \n",
    "        X_component_similarity, \n",
    "        X_rank_sparse])\n",
    "# Create a string array capturing the name of the features -> used later during evaluation\n",
    "feature_names = [\n",
    "    'name_lexical_sim', \n",
    "                 'name_semantic_sim', \n",
    "                 'system_semantic_similarity', \n",
    "                 'component_semantic_similarity', \n",
    "                 'rank']\n",
    "\n",
    "# Labels and query identifiers\n",
    "y = merged_df['relevant'].values\n",
    "# convert query strings to integers -> required for AdaRank\n",
    "merged_df['qid_numeric'] = pd.factorize(merged_df['query'])[0]\n",
    "qid = merged_df['qid_numeric'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('idxdata/indexed_dataset.csv', index=False)\n",
    "# Save the feature matrix X to a file for later retrieval\n",
    "save_npz('idxdata/X_sparse.npz', X)\n",
    "# Save the labels y to a file for later retrieval\n",
    "np.save('idxdata/y.npy', y)\n",
    "# Save the query identifiers qid to a file for later retrieval\n",
    "np.save('idxdata/qid.npy', qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('idxdata/indexed_dataset.csv')\n",
    "feature_names = [\n",
    "    'name_lexical_sim', \n",
    "                 'name_semantic_sim', \n",
    "                 'system_semantic_similarity', \n",
    "                 'component_semantic_similarity', \n",
    "                 'rank']\n",
    "X = load_npz('idxdata/X_sparse.npz')\n",
    "y = np.load('idxdata/y.npy')\n",
    "qid = np.load('idxdata/qid.npy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique queries in train: [1 2 3 4]\n",
      "Unique queries in test: [0]\n",
      "Unique queries and their frequencies: (array([0, 1, 2, 3, 4]), array([ 97,  97,  97, 100,  99]))\n",
      "X_train summary stats (mean, std): [0.07249098 0.23106312 0.0455172  0.16608415 0.13096125] [0.12270613 0.20418464 0.05759793 0.2104454  0.97543962]\n",
      "y_train summary stats (mean, std): 0.1297709923664122 0.3360513084435899\n",
      "Iteration 1: train 1.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (4,) not aligned: 1 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Train and evaluate AdaRank\u001b[39;00m\n\u001b[32m     29\u001b[39m model = AdaRankv2(max_iter=\u001b[32m100\u001b[39m, estop=\u001b[32m10\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m, scorer=NDCGScorer(k=\u001b[32m5\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqid_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# test NNDCG for different values of k\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m20\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/adarankv2.py:67\u001b[39m, in \u001b[36mAdaRankv2.fit\u001b[39m\u001b[34m(self, X, y, qid, X_valid, y_valid, qid_valid)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X.shape[\u001b[32m1\u001b[39m]):\n\u001b[32m     66\u001b[39m     penalty = \u001b[38;5;28mself\u001b[39m.regularization_strength * feature_usage[fid]\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     weighted_perf = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_ranker_perf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m - penalty\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weighted_perf > best_weighted_perf:\n\u001b[32m     69\u001b[39m         best_weighted_perf = weighted_perf\n",
      "\u001b[31mValueError\u001b[39m: shapes (4,1) and (4,) not aligned: 1 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Query-aware train/test splitting\n",
    "# we disbale te random state due to the small dataset\n",
    "splitter = GroupShuffleSplit(test_size=0.2, random_state=None)\n",
    "train_idx, test_idx = next(splitter.split(X, y, groups=qid))\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "qid_train, qid_test = qid[train_idx], qid[test_idx]\n",
    "\n",
    "# Create DataFrames for the train and test sets using the respective indices\n",
    "train_df = merged_df.iloc[train_idx].copy()\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "\n",
    "#Debugging prints\n",
    "print(\"Unique queries in train:\", np.unique(qid[train_idx]))\n",
    "print(\"Unique queries in test:\", np.unique(qid[test_idx]))\n",
    "print(\"Unique queries and their frequencies:\", np.unique(qid, return_counts=True))\n",
    "\n",
    "#print(\"X_train:\\n\", X_train.toarray())\n",
    "#print(\"y_train:\", y_train)\n",
    "\n",
    "#Check basic statistics \n",
    "#We can confirm that all selected features plus label have a meaningful variance/std dev so we dont need to apply a feature selection before \n",
    "print(\"X_train summary stats (mean, std):\", np.mean(X_train.toarray(), axis=0), np.std(X_train.toarray(), axis=0))\n",
    "#print(\"y_train distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"y_train summary stats (mean, std):\", np.mean(y_train, axis=0), np.std(y_train, axis=0))\n",
    "\n",
    "# ------------------------------\n",
    "# Train and evaluate AdaRank\n",
    "model = AdaRankv2(max_iter=100, estop=10, verbose=True, scorer=NDCGScorer(k=5))\n",
    "model.fit(X_train, y_train, qid_train)\n",
    "\n",
    "# test NNDCG for different values of k\n",
    "for k in (1, 2, 3, 4, 5, 10, 20):\n",
    "    y_pred = model.predict(X_test, qid_test) \n",
    "    score = NDCGScorer(k=k)(y_test, y_pred, qid_test).mean()   \n",
    "    print(f\"NDCG Score {score}, K {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique queries in test: 1\n",
      "[4]\n",
      "Number of distinct queries overall: 5\n",
      "['glucose in blood' 'bilirubin in plasma' 'White blood cells count'\n",
      " 'cholesterol in blood' 'PrThr calcium oxalate crystals']\n",
      "Top Predictions per Query:\n",
      "Query: PrThr calcium oxalate crystals\n",
      "                                      long_common_name  y_true    y_pred\n",
      "392  Calcium oxalate dihydrate crystals [Presence] ...       1  3.696418\n",
      "391  Calcium oxalate crystals [Presence] in Urine s...       1  3.554758\n",
      "393  Calcium oxalate crystals [Presence] in Urine b...       1  3.255568\n",
      "394  Calcium oxalate crystals [Presence] in Urine s...       1  3.086415\n",
      "395  Calcium oxalate dihydrate crystals [Presence] ...       1  3.081308\n",
      "398  Calcium oxalate monohydrate crystals [Presence...       1  3.033698\n",
      "396  Calcium oxalate dihydrate crystals [Presence] ...       1  3.001708\n",
      "397  Calcium oxalate dihydrate crystals [Presence] ...       1  2.746387\n",
      "399  Calcium oxalate monohydrate crystals [Presence...       1  2.712515\n",
      "461  Mold Allergen Mix 1 (Alternaria alternata+Aspe...       0  0.000000\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Showcasing some predictions from the test set\n",
    "\n",
    "# Create a DataFrame for the test set using the test indices\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "test_df['y_true'] = y_test\n",
    "test_df['y_pred'] = y_pred\n",
    "\n",
    "#Check how many distinct queries are in the test set\n",
    "unique_test_queries = np.unique(qid_test)\n",
    "print(\"Number of unique queries in test:\", len(unique_test_queries))\n",
    "print(unique_test_queries)\n",
    "\n",
    "all_queries = merged_df['query'].unique()\n",
    "print(\"Number of distinct queries overall:\", len(all_queries))\n",
    "print(all_queries)\n",
    "\n",
    "# for each query in the test set, print the top 10 predictions (sorted by predicted score)\n",
    "# Model seems to predcit well, however could be due to the small dataset and overfitting \n",
    "print(\"Top Predictions per Query:\")\n",
    "for query, group in test_df.groupby('query'):\n",
    "    sorted_group = group.sort_values(by='y_pred', ascending=False)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(sorted_group[['long_common_name', 'y_true', 'y_pred']].head(10))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: name_lexical_sim, Importance (coef): 9.5569\n",
      "Feature: name_semantic_sim, Importance (coef): 0.0000\n",
      "Feature: system_semantic_similarity, Importance (coef): 0.0000\n",
      "Feature: component_semantic_similarity, Importance (coef): 0.0000\n",
      "Feature: rank, Importance (coef): 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Check how the model weights the features \n",
    "coef = model.get_coef\n",
    "# Print feature coefficients explicitly\n",
    "for name, coef in zip(feature_names, model.coef_):\n",
    "    print(f\"Feature: {name}, Importance (coef): {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_query_1 = 'thyroid in blood'\n",
    "val_query_2 = 'MSCnc ser plasma'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
