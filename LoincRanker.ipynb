{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from adarank import AdaRank\n",
    "from adarankv2 import AdaRankv2\n",
    "from metrics import NDCGScorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import save_npz, load_npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#define file path\n",
    "val_file_path = 'Input/loinc_query_terms_testing.xlsx'\n",
    "train_file_path = 'Input/loinc_query_terms_training.xlsx'\n",
    "\n",
    "# Load pretrained model vector embedding model\n",
    "# We are using a medical specific model from huggingface. https://huggingface.co/ls-da3m0ns/bge_large_medical 'ls-da3m0ns/bge_large_medical'\n",
    "# To improve performance, model can be switched to a more general & lightwight model (replace with \"all-MiniLM-L6-v2\")\n",
    "embedder = SentenceTransformer('ls-da3m0ns/bge_large_medical')\n",
    "#embedder = SentenceTransformer('all-MiniLM-L6-v2') # lightweight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely calculate similarity between a query and a field\n",
    "def calculate_embedding_similarity(query, field):\n",
    "    if pd.isna(query) or pd.isna(field):\n",
    "        return 0\n",
    "    query_embedding = embedder.encode([str(query)])[0]\n",
    "    field_embedding = embedder.encode([str(field)])[0]\n",
    "    return cosine_similarity([query_embedding], [field_embedding])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file and merge all sheets into one dataframe\n",
    "xls = pd.ExcelFile(train_file_path)\n",
    "dataframes = []\n",
    "for sheet_name in xls.sheet_names:\n",
    "    temp_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "    temp_df['query'] = sheet_name  # Each sheet name is used as the query text\n",
    "    dataframes.append(temp_df)\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute lexical similarity for long_common_name\n",
    "\n",
    "# We want to compute, for each row, the similarity between the query and the long_common_name.\n",
    "# First, build a TF-IDF vectorizer fitted on the union of all queries and names.\n",
    "corpus = pd.concat([merged_df['query'], merged_df['long_common_name']])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the query and long_common_name columns\n",
    "X_query = vectorizer.transform(merged_df['query'])\n",
    "X_name_tfidf = vectorizer.transform(merged_df['long_common_name'])\n",
    "\n",
    "# Compute cosine similarity for each row\n",
    "cosine_sim = np.array([cosine_similarity(X_query[i], X_name_tfidf[i])[0, 0] \n",
    "                         for i in range(X_query.shape[0])])\n",
    "merged_df['name_cosine_sim'] = cosine_sim\n",
    "\n",
    "# Now use the computed cosine similarity as the feature for the name field.\n",
    "X_name_lexical_similarity = csr_matrix(merged_df[['name_cosine_sim']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute lexical similarity for features property, system, component, long_common_name\n",
    "\n",
    "# Create similarity scores for features, loinc name measurement, system, and component\n",
    "# Apply a similarity function to each row, based on vector embeddings \n",
    "merged_df['name_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['long_common_name']), axis=1)\n",
    "merged_df['property_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['property']), axis=1)\n",
    "merged_df['system_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['system']), axis=1)\n",
    "merged_df['component_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['component']), axis=1)\n",
    "\n",
    "# Convert to sparse matrices\n",
    "X_name_semantic_similarity = csr_matrix(merged_df[['name_similarity']].values)\n",
    "X_property_similarity = csr_matrix(merged_df[['property_similarity']].values)\n",
    "X_system_similarity = csr_matrix(merged_df[['system_similarity']].values)\n",
    "X_component_similarity = csr_matrix(merged_df[['component_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Process numerical feature: 'rank' using StandardScaler\n",
    "# Treat 0 (NaN) values as very high ranks by replacing them with the largest number \n",
    "max_rank = merged_df['rank'].max()\n",
    "merged_df['rank'] = merged_df['rank'].replace(0, max_rank + 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_rank = scaler.fit_transform(merged_df[['rank']])\n",
    "X_rank_sparse = csr_matrix(X_rank)\n",
    "\n",
    "# ------------------------------\n",
    "# Combine all features into one sparse matrix.\n",
    "X = hstack(\n",
    "    [   X_rank_sparse,\n",
    "        X_name_semantic_similarity, \n",
    "        X_name_lexical_similarity,\n",
    "        X_system_similarity, \n",
    "        X_component_similarity\n",
    "        ])\n",
    "\n",
    "# Labels and query identifiers\n",
    "y = merged_df['relevant'].values\n",
    "# convert query strings to integers -> required for AdaRank\n",
    "merged_df['qid_numeric'] = pd.factorize(merged_df['query'])[0]\n",
    "qid = merged_df['qid_numeric'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('idxdata/indexed_dataset.csv', index=False)\n",
    "# Save the feature matrix X to a file for later retrieval\n",
    "save_npz('idxdata/X_sparse.npz', X)\n",
    "# Save the labels y to a file for later retrieval\n",
    "np.save('idxdata/y.npy', y)\n",
    "# Save the query identifiers qid to a file for later retrieval\n",
    "np.save('idxdata/qid.npy', qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = load_npz('idxdata/X_sparse.npz')\n",
    "merged_df = pd.read_csv('idxdata/indexed_dataset.csv')\n",
    "y = np.load('idxdata/y.npy')\n",
    "qid = np.load('idxdata/qid.npy') \n",
    "# Create a string array capturing the name of the features -> used later during evaluation\n",
    "feature_names = ['rank',\n",
    "                 'name_semantic_sim', \n",
    "                 'name_lexical_sim',\n",
    "                 'system_semantic_similarity', \n",
    "                 'component_semantic_similarity', \n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique queries in train: [0 2 3 4]\n",
      "Unique queries in test: [1]\n",
      "Unique queries and their frequencies: (array([0, 1, 2, 3, 4]), array([ 97,  97,  97, 100,  99]))\n",
      "X_train summary stats (mean, std): [0.13366907 0.42985444 0.0666258  0.30803767 0.3964655 ] [0.97326114 0.12833275 0.11768371 0.04521437 0.14848953]\n",
      "y_train summary stats (mean, std): 0.11959287531806616 0.3244848524834767\n",
      "NDCG Score 1.0, K 1\n",
      "NDCG Score 1.0, K 2\n",
      "NDCG Score 1.0, K 3\n",
      "NDCG Score 1.0, K 4\n",
      "NDCG Score 1.0, K 5\n",
      "NDCG Score 1.0, K 10\n",
      "NDCG Score 1.0, K 20\n"
     ]
    }
   ],
   "source": [
    "# Query-aware train/test splitting\n",
    "# we disbale te random state due to the small dataset\n",
    "splitter = GroupShuffleSplit(test_size=0.2, random_state=None)\n",
    "train_idx, test_idx = next(splitter.split(X, y, groups=qid))\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "qid_train, qid_test = qid[train_idx], qid[test_idx]\n",
    "\n",
    "# Create DataFrames for the train and test sets using the respective indices\n",
    "train_df = merged_df.iloc[train_idx].copy()\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "\n",
    "#Debugging prints\n",
    "print(\"Unique queries in train:\", np.unique(qid[train_idx]))\n",
    "print(\"Unique queries in test:\", np.unique(qid[test_idx]))\n",
    "print(\"Unique queries and their frequencies:\", np.unique(qid, return_counts=True))\n",
    "\n",
    "#print(\"X_train:\\n\", X_train.toarray())\n",
    "#print(\"y_train:\", y_train)\n",
    "\n",
    "#Check basic statistics \n",
    "#We can confirm that all selected features plus label have a meaningful variance/std dev so we dont need to apply a feature selection before \n",
    "print(\"X_train summary stats (mean, std):\", np.mean(X_train.toarray(), axis=0), np.std(X_train.toarray(), axis=0))\n",
    "#print(\"y_train distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"y_train summary stats (mean, std):\", np.mean(y_train, axis=0), np.std(y_train, axis=0))\n",
    "\n",
    "# ------------------------------\n",
    "# Train and evaluate AdaRank\n",
    "model = AdaRankv2(max_iter=100, estop=10, scorer=NDCGScorer(k=5))\n",
    "#model = AdaRank(max_iter=100, estop=100, scorer=NDCGScorer(k=5))\n",
    "model.fit(X_train, y_train, qid_train)\n",
    "\n",
    "# test NNDCG for different values of k\n",
    "for k in (1, 2, 3, 4, 5, 10, 20):\n",
    "    y_pred = model.predict(X_test, qid_test) \n",
    "    score = NDCGScorer(k=k)(y_test, y_pred, qid_test).mean()   \n",
    "    print(f\"NDCG Score {score}, K {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique queries in test: 1\n",
      "[1]\n",
      "Number of distinct queries overall: 5\n",
      "['glucose in blood' 'bilirubin in plasma' 'White blood cells count'\n",
      " 'cholesterol in blood' 'PrThr calcium oxalate crystals']\n",
      "Top Predictions per Query:\n",
      "Query: bilirubin in plasma\n",
      "                                      long_common_name  y_true    y_pred\n",
      "172     Bilirubin.direct [Presence] in Serum or Plasma       1  2.425374\n",
      "106  Bilirubin.direct [Mass/volume] in Serum or Plasma       1  2.336588\n",
      "111   Bilirubin.total [Mass/volume] in Serum or Plasma       1  2.322472\n",
      "171  Bilirubin.direct/Bilirubin.total in Serum or P...       1  2.315887\n",
      "125  Bilirubin.indirect [Mass/volume] in Serum or P...       1  2.270974\n",
      "168  Bilirubin.indirect [Mass/volume] in Serum or P...       1  2.270974\n",
      "173  Bilirubin.conjugated/Bilirubin.total in Serum ...       1  2.267906\n",
      "170  Bilirubin.indirect [Moles/volume] in Serum or ...       1  2.253523\n",
      "129  Bilirubin.indirect [Mass or Moles/volume] in S...       1  2.237082\n",
      "169  Bilirubin.indirect [Mass or Moles/volume] in S...       1  2.237082\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Showcasing some predictions from the test set\n",
    "\n",
    "# Create a DataFrame for the test set using the test indices\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "test_df['y_true'] = y_test\n",
    "test_df['y_pred'] = y_pred\n",
    "\n",
    "#Check how many distinct queries are in the test set\n",
    "unique_test_queries = np.unique(qid_test)\n",
    "print(\"Number of unique queries in test:\", len(unique_test_queries))\n",
    "print(unique_test_queries)\n",
    "\n",
    "all_queries = merged_df['query'].unique()\n",
    "print(\"Number of distinct queries overall:\", len(all_queries))\n",
    "print(all_queries)\n",
    "\n",
    "# for each query in the test set, print the top 10 predictions (sorted by predicted score)\n",
    "# Model seems to predcit well, however could be due to the small dataset and overfitting \n",
    "print(\"Top Predictions per Query:\")\n",
    "for query, group in test_df.groupby('query'):\n",
    "    sorted_group = group.sort_values(by='y_pred', ascending=False)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(sorted_group[['long_common_name', 'y_true', 'y_pred']].head(10))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         2.65165245 0.         0.         0.        ]\n",
      "Feature: rank, Importance (coef): 0.0000\n",
      "Feature: name_semantic_sim, Importance (coef): 2.6517\n",
      "Feature: name_lexical_sim, Importance (coef): 0.0000\n",
      "Feature: system_semantic_similarity, Importance (coef): 0.0000\n",
      "Feature: component_semantic_similarity, Importance (coef): 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Check how the model weights the features \n",
    "coef_zip = model.coef_\n",
    "# Print feature coefficients explicitly\n",
    "print(coef_zip)\n",
    "for name, coef in zip(feature_names, coef_zip):\n",
    "    print(f\"Feature: {name}, Importance (coef): {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refactor code to prepare data \n",
    "xls_val = pd.ExcelFile(val_file_path)\n",
    "dataframes_val = []\n",
    "for sheet_name in xls_val.sheet_names:\n",
    "    temp_df = pd.read_excel(xls_val, sheet_name=sheet_name)\n",
    "    temp_df['query'] = sheet_name  # Each sheet name is the query text\n",
    "    dataframes_val.append(temp_df)\n",
    "val_df = pd.concat(dataframes_val, ignore_index=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute Lexical Similarity for 'long_common_name'\n",
    "\n",
    "corpus = pd.concat([val_df['query'], val_df['long_common_name']])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "X_query_val = vectorizer.transform(val_df['query'])\n",
    "X_name_tfidf_val = vectorizer.transform(val_df['long_common_name'])\n",
    "cosine_sim_val = np.array([\n",
    "    cosine_similarity(X_query_val[i], X_name_tfidf_val[i])[0, 0] \n",
    "    for i in range(X_query_val.shape[0])\n",
    "])\n",
    "val_df['name_lexical_similarity'] = cosine_sim_val\n",
    "X_name_lexical_similarity_val = csr_matrix(val_df[['name_lexical_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute Semantic Similarities\n",
    "\n",
    "val_df['name_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['long_common_name']), axis=1)\n",
    "val_df['property_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['property']), axis=1)\n",
    "val_df['system_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['system']), axis=1)\n",
    "val_df['component_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['component']), axis=1)\n",
    "\n",
    "X_name_semantic_similarity_val = csr_matrix(val_df[['name_semantic_similarity']].values)\n",
    "X_property_similarity_val = csr_matrix(val_df[['property_semantic_similarity']].values)\n",
    "X_system_similarity_val = csr_matrix(val_df[['system_semantic_similarity']].values)\n",
    "X_component_similarity_val = csr_matrix(val_df[['component_semantic_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Process Numerical Feature: 'rank'\n",
    "# Replace zeros (or NaN) with a high rank value as before\n",
    "max_rank_val = val_df['rank'].max()\n",
    "val_df['rank'] = val_df['rank'].replace(0, max_rank_val + 1)\n",
    "# Use the same scaler from training.\n",
    "scaler_val = StandardScaler().fit(merged_df[['rank']])\n",
    "X_rank_val = scaler_val.transform(val_df[['rank']])\n",
    "X_rank_sparse_val = csr_matrix(X_rank_val)\n",
    "\n",
    "# ------------------------------\n",
    "# Combine All Features into One Matrix\n",
    "# Use the same ordering as during training.\n",
    "X_val = hstack([\n",
    "    X_rank_sparse_val,\n",
    "    X_name_semantic_similarity_val, \n",
    "    X_name_lexical_similarity_val,\n",
    "    X_system_similarity_val, \n",
    "    X_component_similarity_val\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions:\n",
      "\n",
      "Query: Carbon volume\n",
      "                                      long_common_name    y_pred\n",
      "200        Creatinine [Mass/volume] in Serum or Plasma  1.390240\n",
      "201           Calcium [Mass/volume] in Serum or Plasma  1.421104\n",
      "202  Carbon dioxide, total [Moles/volume] in Serum ...  1.726032\n",
      "203                                   Respiratory rate  1.150598\n",
      "204   Bilirubin.total [Mass/volume] in Serum or Plasma  1.406505\n",
      "..                                                 ...       ...\n",
      "395                  Calcium [Mass/volume] in Specimen  1.550551\n",
      "396  Base excess standard in Arterial blood by calc...  1.203031\n",
      "397  Bacteria identified in Bone by Anaerobe+Aerobe...  0.765750\n",
      "398            Creatinine [Mass/time] in 24 hour Urine  1.057399\n",
      "399         Blasts/Leukocytes in Blood by Manual count  0.938901\n",
      "\n",
      "[200 rows x 2 columns]\n",
      "\n",
      "Query: Creatinine in Blood or Urine\n",
      "                                      long_common_name    y_pred\n",
      "0          Creatinine [Mass/volume] in Serum or Plasma  2.218860\n",
      "1             Calcium [Mass/volume] in Serum or Plasma  1.563589\n",
      "2    Carbon dioxide, total [Moles/volume] in Serum ...  1.432694\n",
      "3                                     Respiratory rate  0.839411\n",
      "4     Bilirubin.total [Mass/volume] in Serum or Plasma  1.318502\n",
      "..                                                 ...       ...\n",
      "195                  Calcium [Mass/volume] in Specimen  1.317066\n",
      "196  Base excess standard in Arterial blood by calc...  1.423546\n",
      "197  Bacteria identified in Bone by Anaerobe+Aerobe...  0.898992\n",
      "198            Creatinine [Mass/time] in 24 hour Urine  2.102097\n",
      "199         Blasts/Leukocytes in Blood by Manual count  1.249271\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "val_df['qid_numeric'] = pd.factorize(val_df['query'])[0]\n",
    "qid_val = val_df['qid_numeric'].values\n",
    "\n",
    "# Predict Using the Fitted AdaRank Model\n",
    "# Assume your AdaRank model has already been trained and is available as 'model'\n",
    "y_val_pred = model.predict(X_val, qid_val)\n",
    "\n",
    "# Add the predictions to the DataFrame for inspection.\n",
    "val_df['y_pred'] = y_val_pred\n",
    "\"\"\" # Print Example Predictions Grouped by Query\n",
    "print(\"Validation Predictions:\")\n",
    "for query, group in val_df.groupby('query'):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    # Adjust column names as needed; here we print the document 'long_common_name' and its prediction.\n",
    "    print(group[['long_common_name', 'y_pred']]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results saved to validation_results_by_query.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Function to assign ranking within each query group\n",
    "def assign_ranking(group):\n",
    "    group = group.sort_values(by='y_pred', ascending=False).copy()\n",
    "    group['AdaRank Ranking'] = range(1, len(group) + 1)\n",
    "    return group\n",
    "\n",
    "# Create a dictionary where each key is a query and value is the ranked DataFrame for that query\n",
    "query_groups = {query: assign_ranking(group) for query, group in val_df.groupby('query')}\n",
    "\n",
    "# Write each query's results to a separate sheet in an Excel file\n",
    "output_file = 'Results/validation_results_by_query.xlsx'\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for query, df_group in query_groups.items():\n",
    "        # Excel sheet names can have a maximum of 31 characters, so we truncate if necessary\n",
    "        sheet_name = query if len(query) <= 31 else query[:31]\n",
    "        df_group.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Validation results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
