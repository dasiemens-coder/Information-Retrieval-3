{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoincRanker Notebook Overview\n",
    "\n",
    "This notebook implements a learning-to-rank pipeline for LOINC data using an AdaRank model.  It covers:\n",
    "- Data merging from multi-sheet Excel files,\n",
    "- Diverse feature extraction techniques (lexical, semantic, and numerical),\n",
    "- Construction of a combined sparse feature matrix,\n",
    "- Query-aware model training and evaluation,\n",
    "- And finally, analysis of model results.\n",
    "\n",
    "The underlying AdaRank model is implemented using the repository git@github.com:rueycheng/AdaRank.git. The AdaRank algorithm was modified to include extra regularization mechnisms. \n",
    "\n",
    "Example predictions can be found in the output of the cells and in the folder Results/.\n",
    "\n",
    "---\n",
    "\n",
    "## Content of notebook \n",
    "\n",
    "1. Data Loading \n",
    "2. Feature Extraction and Data Merging\n",
    "5. Numerical Feature Processing\n",
    "6. Feature Combination\n",
    "7. Labels and Query Identifiers\n",
    "8. Model Training and Evaluation\n",
    "9. Results Analysis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from adarank import AdaRank\n",
    "from adarankv2 import AdaRankv2\n",
    "from metrics import NDCGScorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import save_npz, load_npz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\" val_file_path = 'Input-test/loinc_query_terms_testing.xlsx'\n",
    "train_file_path = 'Input-test/loinc_query_terms_training.xlsx' \"\"\"\n",
    "val_file_path = 'Input/loinc_query_terms_testing.xlsx'\n",
    "train_file_path = 'Input/loinc_query_terms_training.xlsx'\n",
    "\n",
    "# Load pretrained model vector embedding model\n",
    "# We are using a medical specific model from huggingface. https://huggingface.co/ls-da3m0ns/bge_large_medical 'ls-da3m0ns/bge_large_medical'\n",
    "# To improve performance, model can be switched to a more general & lightwight model (replace with \"all-MiniLM-L6-v2\")\n",
    "\n",
    "embedder = SentenceTransformer('ls-da3m0ns/bge_large_medical')\n",
    "#embedder = SentenceTransformer('all-MiniLM-L6-v2') # lightweight model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define function for semantic similarity computation\n",
    "  \n",
    "- Used predefined embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely calculate similarity between a query and a field\n",
    "def calculate_embedding_similarity(query, field):\n",
    "    if pd.isna(query) or pd.isna(field):\n",
    "        return 0\n",
    "    query_embedding = embedder.encode([str(query)])[0]\n",
    "    field_embedding = embedder.encode([str(field)])[0]\n",
    "    return cosine_similarity([query_embedding], [field_embedding])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Loading and Merging\n",
    "- Iterate over all sheet names in the Excel file.\n",
    "- Read each sheet into a temporary DataFrame.\n",
    "- Add a new column (`query`) to capture the sheet name.\n",
    "- Concatenate all the sheets into a single DataFrame (`merged_df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file and merge all sheets into one dataframe\n",
    "xls = pd.ExcelFile(train_file_path)\n",
    "dataframes = []\n",
    "for sheet_name in xls.sheet_names:\n",
    "    temp_df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "    temp_df['query'] = sheet_name  # Each sheet name is used as the query text\n",
    "    dataframes.append(temp_df)\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Compute jaccard similarity for property, system, and component\n",
    "\n",
    "# Define a function to compute the Jaccard similarity between two strings.\n",
    "def jaccard_similarity(str1, str2):\n",
    "    # If either string is not a valid string, return 0.\n",
    "    if not isinstance(str1, str) or not isinstance(str2, str):\n",
    "        return 0.0\n",
    "    set1 = set(str1.lower().split())\n",
    "    set2 = set(str2.lower().split())\n",
    "    union = set1.union(set2)\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    return float(len(set1.intersection(set2))) / len(union)\n",
    "\n",
    "# Compute Jaccard similarity for each of the  columns, comparing the 'query' to each.\n",
    "merged_df['system_jaccard'] = merged_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['system']) if pd.notnull(row['system']) else 0.0, axis=1)\n",
    "merged_df['component_jaccard'] = merged_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['component']) if pd.notnull(row['component']) else 0.0, axis=1)\n",
    "merged_df['property_jaccard'] = merged_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['property']) if pd.notnull(row['property']) else 0.0, axis=1)\n",
    "\n",
    "# inspect the computed similarities (not needed for training)\n",
    "\"\"\" print(\"System Jaccard similarity stats:\")\n",
    "print(merged_df['system_jaccard'].describe())\n",
    "print(\"Component Jaccard similarity stats:\")\n",
    "print(merged_df['component_jaccard'].describe())\n",
    "print(\"Property Jaccard similarity stats:\")\n",
    "print(merged_df['property_jaccard'].describe()) \"\"\"\n",
    "\n",
    "# Convert the columns to sparse matrices so they can be used as features.\n",
    "from scipy.sparse import csr_matrix\n",
    "X_system_jaccard = csr_matrix(merged_df[['system_jaccard']].values)\n",
    "X_component_jaccard = csr_matrix(merged_df[['component_jaccard']].values)\n",
    "X_property_jaccard = csr_matrix(merged_df[['property_jaccard']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute lexical similarity for long_common_name\n",
    "\n",
    "# We want to compute, for each row, the similarity between the query and the long_common_name.\n",
    "# First, build a TF-IDF vectorizer fitted on the union of all queries and names.\n",
    "corpus = pd.concat([merged_df['query'], merged_df['long_common_name']])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Transform the query and long_common_name columns\n",
    "X_query = vectorizer.transform(merged_df['query'])\n",
    "X_name_tfidf = vectorizer.transform(merged_df['long_common_name'])\n",
    "\n",
    "# Compute cosine similarity for each row\n",
    "cosine_sim = np.array([cosine_similarity(X_query[i], X_name_tfidf[i])[0, 0] \n",
    "                         for i in range(X_query.shape[0])])\n",
    "merged_df['name_cosine_sim'] = cosine_sim\n",
    "\n",
    "# Now use the computed cosine similarity as the feature for the name field.\n",
    "X_name_lexical_similarity = csr_matrix(merged_df[['name_cosine_sim']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute semantic similarity for features property, system, component, long_common_name\n",
    "\n",
    "# Create similarity scores for features, loinc name measurement, system, and component\n",
    "# Apply a similarity function to each row, based on vector embeddings \n",
    "merged_df['name_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['long_common_name']), axis=1)\n",
    "merged_df['property_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['property']), axis=1)\n",
    "merged_df['system_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['system']), axis=1)\n",
    "merged_df['component_similarity'] = merged_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['component']), axis=1)\n",
    "\n",
    "# Convert to sparse matrices\n",
    "X_name_semantic_similarity = csr_matrix(merged_df[['name_similarity']].values)\n",
    "X_property_similarity = csr_matrix(merged_df[['property_similarity']].values)\n",
    "X_system_similarity = csr_matrix(merged_df[['system_similarity']].values)\n",
    "X_component_similarity = csr_matrix(merged_df[['component_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Process numerical feature: 'rank' using StandardScaler\n",
    "# Treat 0 (NaN) values as very high ranks by replacing them with the largest number \n",
    "max_rank = merged_df['rank'].max()\n",
    "merged_df['rank'] = merged_df['rank'].replace(0, max_rank + 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_rank = scaler.fit_transform(merged_df[['rank']])\n",
    "X_rank_sparse = csr_matrix(X_rank)\n",
    "\n",
    "# ------------------------------\n",
    "# Combine all features into one sparse matrix.\n",
    "X = hstack([\n",
    "    X_rank_sparse,\n",
    "    X_name_semantic_similarity, \n",
    "    X_name_lexical_similarity,\n",
    "    X_system_similarity, \n",
    "    X_component_similarity,\n",
    "    X_system_jaccard,\n",
    "    X_component_jaccard,\n",
    "    X_property_jaccard\n",
    "])\n",
    "\n",
    "# Labels and query identifiers\n",
    "y = merged_df['relevant'].values\n",
    "# convert query strings to integers -> required for AdaRank\n",
    "merged_df['qid_numeric'] = pd.factorize(merged_df['query'])[0]\n",
    "qid = merged_df['qid_numeric'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('idxdata/indexed_dataset.csv', index=False)\n",
    "# Save the feature matrix X to a file for later retrieval\n",
    "save_npz('idxdata/X_sparse.npz', X)\n",
    "# Save the labels y to a file for later retrieval\n",
    "np.save('idxdata/y.npy', y)\n",
    "# Save the query identifiers qid to a file for later retrieval\n",
    "np.save('idxdata/qid.npy', qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = load_npz('idxdata/X_sparse.npz')\n",
    "merged_df = pd.read_csv('idxdata/indexed_dataset.csv')\n",
    "y = np.load('idxdata/y.npy')\n",
    "qid = np.load('idxdata/qid.npy') \n",
    "# Create a string array capturing the name of the features -> used later during evaluation\n",
    "feature_names = [\n",
    "    \"X_rank_sparse\",\n",
    "    \"X_name_semantic_similarity\", \n",
    "    \"X_name_lexical_similarity\",\n",
    "    \"X_system_similarity\", \n",
    "    \"X_component_similarity\",\n",
    "    \"X_system_jaccard\",\n",
    "    \"X_component_jaccard\",\n",
    "    \"X_property_jaccard\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique queries in train: [0 1 3 5 6 7]\n",
      "Unique queries in test: [2 4]\n",
      "Unique queries and their frequencies: (array([0, 1, 2, 3, 4, 5, 6, 7]), array([517, 491, 517, 514, 550, 521, 550, 550]))\n",
      "X_train summary stats (mean, std): [0.0056889  0.39012529 0.0403473  0.36812505 0.38595793 0.00403012\n",
      " 0.01871622 0.05472479] [0.95613794 0.11574428 0.08996267 0.08048696 0.12437721 0.03642981\n",
      " 0.1132546  0.22744226]\n",
      "y_train summary stats (mean, std): 0.10976773783009863 0.31260003448778023\n",
      "NDCG Score 1.0, K 1\n",
      "NDCG Score 1.0, K 2\n",
      "NDCG Score 1.0, K 3\n",
      "NDCG Score 1.0, K 4\n",
      "NDCG Score 1.0, K 5\n",
      "NDCG Score 1.0, K 10\n",
      "NDCG Score 1.0, K 20\n"
     ]
    }
   ],
   "source": [
    "# Query-aware train/test splitting\n",
    "# we disbale te random state due to the small dataset\n",
    "splitter = GroupShuffleSplit(test_size=0.2, random_state=None)\n",
    "train_idx, test_idx = next(splitter.split(X, y, groups=qid))\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "qid_train, qid_test = qid[train_idx], qid[test_idx]\n",
    "\n",
    "# Create DataFrames for the train and test sets using the respective indices\n",
    "train_df = merged_df.iloc[train_idx].copy()\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "\n",
    "#Debugging prints\n",
    "print(\"Unique queries in train:\", np.unique(qid[train_idx]))\n",
    "print(\"Unique queries in test:\", np.unique(qid[test_idx]))\n",
    "print(\"Unique queries and their frequencies:\", np.unique(qid, return_counts=True))\n",
    "\n",
    "#print(\"X_train:\\n\", X_train.toarray())\n",
    "#print(\"y_train:\", y_train)\n",
    "\n",
    "#Check basic statistics \n",
    "#We can confirm that all selected features plus label have a meaningful variance/std dev so we dont need to apply a feature selection before \n",
    "print(\"X_train summary stats (mean, std):\", np.mean(X_train.toarray(), axis=0), np.std(X_train.toarray(), axis=0))\n",
    "#print(\"y_train distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"y_train summary stats (mean, std):\", np.mean(y_train, axis=0), np.std(y_train, axis=0))\n",
    "\n",
    "# ------------------------------\n",
    "# Train and evaluate AdaRank\n",
    "model = AdaRankv2(max_iter=500, estop=50, scorer=NDCGScorer(k=5))\n",
    "#model = AdaRank(max_iter=100, estop=100, scorer=NDCGScorer(k=5))\n",
    "model.fit(X_train, y_train, qid_train)\n",
    "\n",
    "# test NNDCG for different values of k\n",
    "for k in (1, 2, 3, 4, 5, 10, 20):\n",
    "    y_pred = model.predict(X_test, qid_test) \n",
    "    score = NDCGScorer(k=k)(y_test, y_pred, qid_test).mean()   \n",
    "    print(f\"NDCG Score {score}, K {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique queries in test: 2\n",
      "[2 4]\n",
      "Number of distinct queries overall: 8\n",
      "['glucose in blood' 'bilirubin in plasma' 'White blood cells count'\n",
      " 'cholesterol in Bld' 'fever virus' 'calcium oxalate crystals' 'iron'\n",
      " 'PrThr']\n",
      "Top Predictions per Query:\n",
      "Query: White blood cells count\n",
      "                                       long_common_name  y_true    y_pred\n",
      "1098  Blasts/Leukocytes [Pure number fraction] in Bo...       1  4.701988\n",
      "1088  Deprecated Myeloblasts/100 leukocytes in Blood...       1  4.667403\n",
      "1122  Myelocytes/Leukocytes in Stem cell product by ...       1  4.658710\n",
      "1112  Deprecated Monocytes+Macrophages/100 leukocyte...       1  4.655984\n",
      "1097          Large unstained cells/Leukocytes in Blood       1  4.650530\n",
      "1079          Nonhematic cells/Leukocytes in Body fluid       1  4.629085\n",
      "1121  Eosinophils/Leukocytes in Stem cell product by...       1  4.628964\n",
      "1105                    Heterophils/Leukocytes in Blood       1  4.614963\n",
      "1095  Other cells/Leukocytes in Synovial fluid by Ma...       1  4.610029\n",
      "1081  Neutrophils/Leukocytes [Pure number fraction] ...       1  4.604194\n",
      "----------------------------------------\n",
      "Query: fever virus\n",
      "                                       long_common_name  y_true    y_pred\n",
      "2083  Colorado tick fever virus Ab [Presence] in Spe...       1  4.585992\n",
      "2046  Rift valley fever virus Ag [Presence] in Specimen       1  4.575512\n",
      "2076     Rift valley fever virus Ab [Presence] in Serum       1  4.572767\n",
      "2075  Colorado tick fever virus RNA [Presence] in Sp...       1  4.569263\n",
      "2052  African swine fever virus Ab [Presence] in Bod...       1  4.553313\n",
      "2082  African swine fever virus Ag [Presence] in Tis...       1  4.550629\n",
      "2042  Colorado tick fever virus IgG Ab [Presence] in...       1  4.549562\n",
      "2085          Yellow fever virus Ab [Presence] in Serum       1  4.547721\n",
      "2078  Crimean-Congo hemorrhagic fever virus RNA [Pre...       1  4.547554\n",
      "2059  Classical swine fever virus neutralizing antib...       1  4.542923\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Showcasing some predictions from the test set\n",
    "\n",
    "# Create a DataFrame for the test set using the test indices\n",
    "test_df = merged_df.iloc[test_idx].copy()\n",
    "test_df['y_true'] = y_test\n",
    "test_df['y_pred'] = y_pred\n",
    "\n",
    "#Check how many distinct queries are in the test set\n",
    "unique_test_queries = np.unique(qid_test)\n",
    "print(\"Number of unique queries in test:\", len(unique_test_queries))\n",
    "print(unique_test_queries)\n",
    "\n",
    "all_queries = merged_df['query'].unique()\n",
    "print(\"Number of distinct queries overall:\", len(all_queries))\n",
    "print(all_queries)\n",
    "\n",
    "# for each query in the test set, print the top 10 predictions (sorted by predicted score)\n",
    "# Model seems to predcit well, however could be due to the small dataset and overfitting \n",
    "print(\"Top Predictions per Query:\")\n",
    "for query, group in test_df.groupby('query'):\n",
    "    sorted_group = group.sort_values(by='y_pred', ascending=False)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(sorted_group[['long_common_name', 'y_true', 'y_pred']].head(10))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.20870325 1.23835749 0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Feature: X_rank_sparse, Importance (coef): 1.2087\n",
      "Feature: X_name_semantic_similarity, Importance (coef): 1.2384\n",
      "Feature: X_name_lexical_similarity, Importance (coef): 0.0000\n",
      "Feature: X_system_similarity, Importance (coef): 0.0000\n",
      "Feature: X_component_similarity, Importance (coef): 0.0000\n",
      "Feature: X_system_jaccard, Importance (coef): 0.0000\n",
      "Feature: X_component_jaccard, Importance (coef): 0.0000\n",
      "Feature: X_property_jaccard, Importance (coef): 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Check how the model weights the features \n",
    "coef_zip = model.coef_\n",
    "# Print feature coefficients explicitly\n",
    "print(coef_zip)\n",
    "for name, coef in zip(feature_names, coef_zip):\n",
    "    print(f\"Feature: {name}, Importance (coef): {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     47\u001b[39m val_df[\u001b[33m'\u001b[39m\u001b[33mproperty_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m] = val_df.apply(\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: calculate_embedding_similarity(row[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33mproperty\u001b[39m\u001b[33m'\u001b[39m]), axis=\u001b[32m1\u001b[39m)\n\u001b[32m     49\u001b[39m val_df[\u001b[33m'\u001b[39m\u001b[33msystem_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m] = val_df.apply(\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: calculate_embedding_similarity(row[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m]), axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m val_df[\u001b[33m'\u001b[39m\u001b[33mcomponent_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mval_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalculate_embedding_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcomponent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m X_name_semantic_similarity_val = csr_matrix(val_df[[\u001b[33m'\u001b[39m\u001b[33mname_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m]].values)\n\u001b[32m     55\u001b[39m X_property_similarity_val = csr_matrix(val_df[[\u001b[33m'\u001b[39m\u001b[33mproperty_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m]].values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/pandas/core/frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/pandas/core/apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     47\u001b[39m val_df[\u001b[33m'\u001b[39m\u001b[33mproperty_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m] = val_df.apply(\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: calculate_embedding_similarity(row[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33mproperty\u001b[39m\u001b[33m'\u001b[39m]), axis=\u001b[32m1\u001b[39m)\n\u001b[32m     49\u001b[39m val_df[\u001b[33m'\u001b[39m\u001b[33msystem_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m] = val_df.apply(\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: calculate_embedding_similarity(row[\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m]), axis=\u001b[32m1\u001b[39m)\n\u001b[32m     51\u001b[39m val_df[\u001b[33m'\u001b[39m\u001b[33mcomponent_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m] = val_df.apply(\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mcalculate_embedding_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcomponent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     54\u001b[39m X_name_semantic_similarity_val = csr_matrix(val_df[[\u001b[33m'\u001b[39m\u001b[33mname_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m]].values)\n\u001b[32m     55\u001b[39m X_property_similarity_val = csr_matrix(val_df[[\u001b[33m'\u001b[39m\u001b[33mproperty_semantic_similarity\u001b[39m\u001b[33m'\u001b[39m]].values)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mcalculate_embedding_similarity\u001b[39m\u001b[34m(query, field)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isna(query) \u001b[38;5;129;01mor\u001b[39;00m pd.isna(field):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m query_embedding = \u001b[43membedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m field_embedding = embedder.encode([\u001b[38;5;28mstr\u001b[39m(field)])[\u001b[32m0\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cosine_similarity([query_embedding], [field_embedding])[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m features.update(extra_features)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    625\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    688\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    689\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/sentence_transformers/models/Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    574\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    575\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    584\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    594\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:524\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    506\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    507\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    514\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    515\u001b[39m     self_outputs = \u001b[38;5;28mself\u001b[39m.self(\n\u001b[32m    516\u001b[39m         hidden_states,\n\u001b[32m    517\u001b[39m         attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    522\u001b[39m         output_attentions,\n\u001b[32m    523\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    525\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/transformers/models/bert/modeling_bert.py:467\u001b[39m, in \u001b[36mBertSelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m    466\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n\u001b[32m    469\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VSCode/Information-Retrieval-3/.venv/lib/python3.13/site-packages/torch/_VF.py:27\u001b[39m, in \u001b[36mVFModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(name)\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m.vf = torch._C._VariableFunctions\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.vf, name)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Refactor code to prepare data \n",
    "xls_val = pd.ExcelFile(val_file_path)\n",
    "dataframes_val = []\n",
    "for sheet_name in xls_val.sheet_names:\n",
    "    temp_df = pd.read_excel(xls_val, sheet_name=sheet_name)\n",
    "    temp_df['query'] = sheet_name  # Each sheet name is the query text\n",
    "    dataframes_val.append(temp_df)\n",
    "val_df = pd.concat(dataframes_val, ignore_index=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute Lexical Similarity for 'long_common_name'\n",
    "\n",
    "corpus = pd.concat([val_df['query'], val_df['long_common_name']])\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "X_query_val = vectorizer.transform(val_df['query'])\n",
    "X_name_tfidf_val = vectorizer.transform(val_df['long_common_name'])\n",
    "cosine_sim_val = np.array([\n",
    "    cosine_similarity(X_query_val[i], X_name_tfidf_val[i])[0, 0] \n",
    "    for i in range(X_query_val.shape[0])\n",
    "])\n",
    "val_df['name_lexical_similarity'] = cosine_sim_val\n",
    "X_name_lexical_similarity_val = csr_matrix(val_df[['name_lexical_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Compute Lexical Similarity \n",
    "\n",
    "\n",
    "# Compute Jaccard similarity for each of the  columns, comparing the 'query' to each.\n",
    "val_df['component_jaccard'] = val_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['component']) if pd.notnull(row['component']) else 0.0, axis=1)\n",
    "val_df['system_jaccard'] = val_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['system']) if pd.notnull(row['system']) else 0.0, axis=1)\n",
    "val_df['property_jaccard'] = val_df.apply(\n",
    "    lambda row: jaccard_similarity(row['query'], row['property']) if pd.notnull(row['property']) else 0.0, axis=1)\n",
    "\n",
    "# Convert the columns to sparse matrices so they can be used as features.\n",
    "from scipy.sparse import csr_matrix\n",
    "X_component_jaccard_val = csr_matrix(val_df[['component_jaccard']].values)\n",
    "X_system_jaccard_val = csr_matrix(val_df[['system_jaccard']].values)\n",
    "X_property_jaccard_val = csr_matrix(val_df[['property_jaccard']].values)\n",
    "# ------------------------------\n",
    "# Compute Semantic Similarities\n",
    "\n",
    "val_df['name_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['long_common_name']), axis=1)\n",
    "val_df['property_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['property']), axis=1)\n",
    "val_df['system_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['system']), axis=1)\n",
    "val_df['component_semantic_similarity'] = val_df.apply(\n",
    "    lambda row: calculate_embedding_similarity(row['query'], row['component']), axis=1)\n",
    "\n",
    "X_name_semantic_similarity_val = csr_matrix(val_df[['name_semantic_similarity']].values)\n",
    "X_property_similarity_val = csr_matrix(val_df[['property_semantic_similarity']].values)\n",
    "X_system_similarity_val = csr_matrix(val_df[['system_semantic_similarity']].values)\n",
    "X_component_similarity_val = csr_matrix(val_df[['component_semantic_similarity']].values)\n",
    "\n",
    "# ------------------------------\n",
    "# Process Numerical Feature: 'rank'\n",
    "# Replace zeros (or NaN) with a high rank value as before\n",
    "max_rank_val = val_df['rank'].max()\n",
    "val_df['rank'] = val_df['rank'].replace(0, max_rank_val + 1)\n",
    "# Use the same scaler from training.\n",
    "scaler_val = StandardScaler().fit(merged_df[['rank']])\n",
    "X_rank_val = scaler_val.transform(val_df[['rank']])\n",
    "X_rank_sparse_val = csr_matrix(X_rank_val)\n",
    "\n",
    "# ------------------------------\n",
    "# Combine All Features into One Matrix\n",
    "# Use the same ordering as during training.\n",
    "X_val = hstack([\n",
    "    X_rank_sparse_val,\n",
    "    X_name_semantic_similarity_val, \n",
    "    X_name_lexical_similarity_val,\n",
    "    X_system_similarity_val, \n",
    "    X_component_similarity_val,\n",
    "    X_system_jaccard_val,\n",
    "    X_component_jaccard_val,\n",
    "    X_property_jaccard_val\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions:\n",
      "\n",
      "Query: Creatinine Urine or Blood\n",
      "                                     long_common_name  y_pred\n",
      "0   11-Deoxycortisol [Mass/volume] in Serum or Plasma     0.0\n",
      "1   Hemoglobin.gastrointestinal [Presence] in Vomitus     0.0\n",
      "2           Busulfan [Mass/volume] in Serum or Plasma     0.0\n",
      "3   Fetal Trisomy 13 risk [Likelihood] based on Pl...     0.0\n",
      "4   Cholesterol esters/Cholesterol.total in Serum ...     0.0\n",
      "5    Bacteria identified in Isolate by Aerobe culture     0.0\n",
      "6                   Meperidine [Presence] in Specimen     0.0\n",
      "7           Green Bean IgE Ab [Units/volume] in Serum     0.0\n",
      "8       Japanese Cedar IgE Ab [Units/volume] in Serum     0.0\n",
      "9   Chronic lymphocytic leukemia gene targeted mut...     0.0\n",
      "10                              Cell type in Specimen     0.0\n",
      "11  Drugs identified in Blood by Screen method Nom...     0.0\n",
      "12  California Live Oak IgE Ab [Units/volume] in S...     0.0\n",
      "13  Entamoeba histolytica DNA [Presence] in Stool ...     0.0\n",
      "14  Calcium sulfate crystals [Presence] in Urine s...     0.0\n",
      "15  Alpha hydroxyalprazolam [Mass/volume] in Urine...     0.0\n",
      "16  Cholesterol non HDL [Moles/volume] in Serum or...     0.0\n",
      "17  Purkinje cells Ab [Presence] in Serum by Immun...     0.0\n",
      "18  Duck feather IgE Ab [Presence] in Serum by Rad...     0.0\n",
      "19                      dilTIAZem [Presence] in Urine     0.0\n",
      "20                      cloZAPine [Presence] in Urine     0.0\n"
     ]
    }
   ],
   "source": [
    "val_df['qid_numeric'] = pd.factorize(val_df['query'])[0]\n",
    "qid_val = val_df['qid_numeric'].values\n",
    "\n",
    "# Predict Using the Fitted AdaRank Model\n",
    "# Assume your AdaRank model has already been trained and is available as 'model'\n",
    "y_val_pred = model.predict(X_val, qid_val)\n",
    "\n",
    "# Add the predictions to the DataFrame for inspection.\n",
    "val_df['y_pred'] = y_val_pred\n",
    "# Print Example Predictions Grouped by Query\n",
    "print(\"Validation Predictions:\")\n",
    "for query, group in val_df.groupby('query'):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    # Adjust column names as needed; here we print the document 'long_common_name' and its prediction.\n",
    "    print(group[['long_common_name', 'y_pred']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Predictions:\n",
      "\n",
      "Query: Creatinine Urine or Blood\n",
      "                                     long_common_name  y_pred\n",
      "0   11-Deoxycortisol [Mass/volume] in Serum or Plasma     0.0\n",
      "1   Hemoglobin.gastrointestinal [Presence] in Vomitus     0.0\n",
      "2           Busulfan [Mass/volume] in Serum or Plasma     0.0\n",
      "3   Fetal Trisomy 13 risk [Likelihood] based on Pl...     0.0\n",
      "4   Cholesterol esters/Cholesterol.total in Serum ...     0.0\n",
      "5    Bacteria identified in Isolate by Aerobe culture     0.0\n",
      "6                   Meperidine [Presence] in Specimen     0.0\n",
      "7           Green Bean IgE Ab [Units/volume] in Serum     0.0\n",
      "8       Japanese Cedar IgE Ab [Units/volume] in Serum     0.0\n",
      "9   Chronic lymphocytic leukemia gene targeted mut...     0.0\n",
      "10                              Cell type in Specimen     0.0\n",
      "11  Drugs identified in Blood by Screen method Nom...     0.0\n",
      "12  California Live Oak IgE Ab [Units/volume] in S...     0.0\n",
      "13  Entamoeba histolytica DNA [Presence] in Stool ...     0.0\n",
      "14  Calcium sulfate crystals [Presence] in Urine s...     0.0\n",
      "15  Alpha hydroxyalprazolam [Mass/volume] in Urine...     0.0\n",
      "16  Cholesterol non HDL [Moles/volume] in Serum or...     0.0\n",
      "17  Purkinje cells Ab [Presence] in Serum by Immun...     0.0\n",
      "18  Duck feather IgE Ab [Presence] in Serum by Rad...     0.0\n",
      "19                      dilTIAZem [Presence] in Urine     0.0\n",
      "20                      cloZAPine [Presence] in Urine     0.0\n"
     ]
    }
   ],
   "source": [
    "val_df['qid_numeric'] = pd.factorize(val_df['query'])[0]\n",
    "qid_val = val_df['qid_numeric'].values\n",
    "\n",
    "# Predict Using the Fitted AdaRank Model\n",
    "# Assume your AdaRank model has already been trained and is available as 'model'\n",
    "y_val_pred = model.predict(X_val, qid_val)\n",
    "\n",
    "# Add the predictions to the DataFrame for inspection.\n",
    "val_df['y_pred'] = y_val_pred\n",
    "# Print Example Predictions Grouped by Query\n",
    "print(\"Validation Predictions:\")\n",
    "for query, group in val_df.groupby('query'):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    # Adjust column names as needed; here we print the document 'long_common_name' and its prediction.\n",
    "    print(group[['long_common_name', 'y_pred']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results saved to Results/validation_results_by_query.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Function to assign ranking within each query group\n",
    "def assign_ranking(group):\n",
    "    group = group.sort_values(by='y_pred', ascending=False).copy()\n",
    "    group['AdaRank Ranking'] = range(1, len(group) + 1)\n",
    "    return group\n",
    "\n",
    "# Create a dictionary where each key is a query and value is the ranked DataFrame for that query\n",
    "query_groups = {query: assign_ranking(group) for query, group in val_df.groupby('query')}\n",
    "\n",
    "# Write each query's results to a separate sheet in an Excel file\n",
    "output_file = 'Results/validation_results_by_query.xlsx'\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for query, df_group in query_groups.items():\n",
    "        # Excel sheet names can have a maximum of 31 characters, so we truncate if necessary\n",
    "        sheet_name = query if len(query) <= 31 else query[:31]\n",
    "        df_group.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Validation results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
